# 梯度下降法

- 不是一个机器学习算法
- 是一个基于搜索的最优化方法
- 作用：最小化一个损失函数
- 梯度上升法，最大化一个效用函数
- η称为学习率
- η的取值影响获得最优解的速度
- η取值不合适，甚至得不到最优解
- η是梯度下降法的一个超参数
- 并不是所有函数都有唯一的极值点

没有唯一的极值点的解决方案：

- 多次运行，随机化初始点
- 梯度下降法的初始点也是一个超参数

## 公式表达

原损失函数为：

$$
 J=\sum_{i=1}^m (y^{(i)} - \hat{y}^{(i)})^2 \\

-\eta \frac{dJ}{d\theta}  导数代表方向\\

\theta = (\theta_0,\theta_1,\dots, \theta_n)
\\
转换为 - \eta \nabla J \qquad 其中 \nabla J = (\frac{\partial J}{\partial \theta_0}, \frac{\partial J}{\partial \theta_1}, \dotsc, \frac{\partial J}{\partial \theta_n}) 梯度代表方向
$$

$$
目标：使 \frac{1}{m} \sum_{i=1}^m(y^{i} - \theta_0 - \theta_1X_1^{(i)} - \theta_2X_2^{(i)} - \dots - \theta_nX_n^{(i)})^2 尽可能小
$$

$$
\nabla J(\theta) = \Bigg(\begin{matrix}
  \frac{\partial J}{\theta_1}  \\
  \frac{\partial J}{\theta_2}  \\
  \frac{\partial J}{\theta_3}  \\
  \dots \\
  \frac{\partial J}{\theta_n}  \\
  \end{matrix}\Bigg) = \Biggl(\begin{matrix}
    \sum_{i=1}^m2(y^{(i)} - X_b^{(i)}\theta) \cdot (-1) \\
    \sum_{i=1}^m2(y^{(i)} - X_b^{(i)}\theta) \cdot (-X_1^{(i)}) \\
    \sum_{i=1}^m2(y^{(i)} - X_b^{(i)}\theta) \cdot (-X_2^{(i)}) \\
    \dots \\
    \sum_{i=1}^m2(y^{(i)} - X_b^{(i)}\theta) \cdot (-X_n^{(i)}) \\
  \end{matrix}\Biggl) = \frac{2}{m} \cdot \Biggl(\begin{matrix}
    \sum_{i=1}^m( X_b^{(i)}\theta - y^{(i)})  \\
    \sum_{i=1}^m( X_b^{(i)}\theta - y^{(i)}) . X_1^{(i)} \\
    \sum_{i=1}^m( X_b^{(i)}\theta - y^{(i)}) . X_2^{(i)} \\
    \dots \\
    \sum_{i=1}^m( X_b^{(i)}\theta - y^{(i)}) . X_n^{(i)} \\
  \end{matrix}\Biggl) = \frac{2}{m}\cdot(X_b\theta - y)^T\cdot X_b = \frac{2}{m}\cdot X_b^T\cdot(X_b\theta - y)
$$
转换为目标函数为
$$
J(\theta)=MSE(y,\hat{y})=\frac{1}{m} \sum_{i=1}^m (y^{(i)} - \hat{y}^{(i)})^2
$$

## 随机梯度下降法

Stochastic Gradient Descent

模拟退火的思想
$$
\eta = \frac{t_0}{i\_iters + t_1}\\
t_0和t_1是超参数常量，经验数值是5 和50
$$