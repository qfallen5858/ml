# 什么是决策树

- 非参数学习算法
- 可以解决分类问题
- 天然可以解决多分类问题
- 也可以解决回归问题
- 非常好的可解释性

## 构建决策树

关键问题

- 每个节点在哪个维度做划分
- 某个维度在哪个值上做划分

### 信息熵

熵在信息论中代表随机变量不确定度的度量

- 熵越大，数据的不确定性越高
- 熵越小，数据的不确定性越低

$$
H = -\sum_{i = 1}^kp_i\log(p_i) \qquad p_i第i类样本占样本总数的比例 \\
\{\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\} \qquad H = 1.0986 \\
\{\frac{1}{10}, \frac{2}{10}, \frac{7}{10}\} \qquad H=0.8018\\
\{1, 0, 0\} \qquad H=0\\

\\
二分类问题转变为 H = -xlog(x) - (1-x)log(1-x)
$$

所以构建决策树，在节点划分后使得信息熵降低，遍历节点搜索来确定划分

### 基尼系数

$$
G = 1 - \sum_{i=1}^kp_i^2 \\
\{\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\} \qquad G = 0.6666 \\
\{\frac{1}{10}, \frac{2}{10}, \frac{7}{10}\} \qquad G=0.46\\
\{1, 0, 0\} \qquad G=0\\
$$

基尼系数在信息论中也代表随机变量不确定度的度量

- 基尼系数越大，数据的不确定性越高
- 基尼系数越小，数据的不确定性越低

$$
二分类问题转变为 G = 1- x^2 -(1-x)^2 = -2x^2 + 2x 在x = 0.5时有最大值
$$

### 信息熵VS基尼系数

- 熵信息的计算比基尼系数稍慢
- scikit-learn中默认为基尼系数
- 大多数时候二者没有特别的效果优劣

### CART 

Classification And Regression Tree

根据某一个维度d 和某一个阈值v进行二分

scikit-learn的决策树实现采用CART

还有其他的实现方式ID3, C4.5, C5.0

### 复杂度

预测： O(logm)

训练：O(n * m * logm)

剪枝：降低复杂度，解决过拟合

决策树的超参数

- min_samples_split 至少有多少个样本才拆分
- max_depth 拆分的树高
- min_samples_leaf 叶节点最少的样本数
- max_leaf_nodes 最多拆分出多少个叶节点
- min_weight_fraction_leaf
- min_features

## 决策树的局限性

- 每次拆分都是单维拆分，所以画出来的决策边界是与坐标轴平行的折现
- 对个别数据极度敏感