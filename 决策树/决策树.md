# 什么是决策树

- 非参数学习算法
- 可以解决分类问题
- 天然可以解决多分类问题
- 也可以解决回归问题
- 非常好的可解释性

## 构建决策树

关键问题

- 每个节点在哪个维度做划分
- 某个维度在哪个值上做划分

熵在信息论中代表随机变量不确定度的度量

- 熵越大，数据的不确定性越高
- 熵越小，数据的不确定性越低

$$
H = -\sum_{i = 1}^kp_i\log(p_i) \qquad p_i第i类样本占样本总数的比例 \\
\{\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\} \qquad H = 1.0986 \\
\{\frac{1}{10}, \frac{2}{10}, \frac{7}{10}\} \qquad H=0.8018\\
\{1, 0, 0\} \qquad H=0\\

\\
二分类问题转变为 H = -xlog(x) - (1-x)log(1-x)
$$

所以构建决策树，在节点划分后使得信息熵降低，遍历节点搜索来确定划分