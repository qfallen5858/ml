# 分类准确度的问题

对于极度偏斜(Skewed Data)的数据，只使用分类准确度是远远不够的，需要使用混淆矩阵做进一步的分析

## 混淆矩阵 Confusion Matrix

对于二分类问题
行代表真实值，列代表预测值

0 - Negative   1- Positive

| 真实\预测 | 0| 1|
|:-----:|:-----:|:-----:|
|0 | TN(预测negative正确) | FP（预测positive错误） |
|1 | FN(预测negative错误) | TP（预测positive正确）|


10000个病人预测是否有癌症

| 真实\预测 | 0| 1|
|:-----:|:-----:|:-----:|
|0 | 9978(TN) | 12(FP) |
|1 | 2(FN) | 8(TP)|

精准率
$precision = \frac{TP}{TP+FP} = \frac{8}{8+12}=40\%$ 被判断为发生的预测中，有多少是准确的

召回率
$recall=\frac{TP}{TP+FN} = \frac{8}{8+2}$ 真实发生的实施，有多少被预测出来的比率

### 精确率和召回率

有10000个病人，我们预测所有人都是健康的，实际患病比例为0.1%

| 真实\预测 | 0| 1|
|:-----:|:-----:|:-----:|
|0 | 9990 | 0 |
|1 | 10 | 0|

准确率$=\frac{9990}{10000}=99.9\%$

精确率$=\frac{0}{0+0}=无意义$

召回率$=\frac{0}{0+10} = 0$

有时候我们注重精准率，如股票预测(升为1，降为0)，却相对忽视召回率

有的时候我们注重召回率，如病人诊断， 尽可能把实施上患病的人群找出来，对于误诊有病的可以通过后续测试排除

## F1 Score

兼顾精准率和召回率, F1 Score是precision和recall的调和平均值

$$
\frac{1}{F_1} = \frac{1}{2}(\frac{1}{precisions}+\frac{1}{recall}) \\  \\
F1 = \frac{2\cdot precision \cdot recall}{precision + recall}
$$

## Precision-Recall的平衡

通常 精确率和召回率此消彼长，可以用P-R曲线与坐标轴之间所夹面积的大小来判断模型的好坏

### ROC 曲线

Receiver Operation Characteristic Curve 描述TPR和FPR之间的关系

TPR = Recall = $\frac{TP}{TP + FP}$

FPR = $\frac{FP}{TN+FP}$ 预测为1且实际为0的样本数，占非0的样本数比例

TPR 和 FPR 增长同方向, 也可以用所夹面价的大小来判断模型的好坏，称为ROC-AUC指标