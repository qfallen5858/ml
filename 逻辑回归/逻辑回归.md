# 什么是逻辑回归

逻辑回归 Logistic Regression

逻辑回归：解决分类问题。


把样本的特征和样本发生的概率联系起来，概率是一个数

$$
\hat{y} = f(x)\\

\hat{p} = f(x) \space \hat{y} = \begin{cases}
  1, & \hat{p}\geq0.5\\
  0, & \hat{p}\leq0.5
\end{cases}
$$

逻辑回归既可以看作是回归算法，也可以看作是分类算法。通常作为分类算法用，只可以解决二分类问题

## Sigmoid函数

$$
\hat{p} = \sigma(\theta^T x) \\
\sigma(t) = \frac{1}{1+e^{-t}} \\
 值域为(0,1), t>0 时， p>0.5, t<0时，p<0.5
$$

$$
\hat{y} = f(x) = \theta^T\cdot x_b \\
\hat{p} = \sigma(\theta^T\cdot x_b) = \frac{1}{ 1 + e^{-\theta^T\cdot x_b}}\\
\hat{y}  = \begin{cases}
  1, & \hat{p}\geq0.5\\
  0, & \hat{p}\leq0.5
\end{cases}
$$

问题转化为：对于给定的样本数据集X,y,如何找到参数theta,使得用这样的方式，可以最大程度获得样本数据集X对应的分类输出y，即尽可能拟合训练集

## 定义逻辑回归的损失函数
$$
\hat{y}  = \begin{cases}
  1, & \hat{p}\geq0.5\\
  0, & \hat{p}\leq0.5
\end{cases}

\\
cost  = \begin{cases}
  如果y=1, p 越小, cost越大\\
  如果y=0, p越大, cost越大
\end{cases}
\\
cost  = \begin{cases}
  -\log(\hat{p}), & if \qquad y = 1\\
  -\log(1-\hat{p}), & if \qquad y = 0
\end{cases}
\\
cos = -y\log(\hat{p}) - (1-y)\log(1-\hat{p})
\\
J(\theta) = -\frac{1}{m}\sum_{i=1}^m(y^{(i)}\log(\hat{p}^{(i)}) + (1-y^{(i)})\log(1-\hat{p}^{(i)}))
\\
\hat{p}^{(i)} = \sigma(X^{(i)}_b \theta) = \frac{1}{1+e^{-X^{(i)}_b \theta}}
$$
没有公式解，只能使用梯度下降法求解


## 推导过程
$$
\sigma(t) = \frac{1}{1+e^{-t}} = (1+e^{-t})^{-1} \\
(\log\sigma(t))^{'} = (1+e^{-t})^{-1}\cdot e^{-t} = \cfrac{e^{-t}}{1+e^{-t}} = \cfrac{1+e^{-t}-1}{1+e^{-t}} = 1-\frac{1}{1+e^{-t}} = 1- \sigma(t) \\
(\log(1-\sigma(t)))^{'} = -\sigma(t)
$$

推导出
$$
\frac{d(y^{(i)}\log\sigma(X^{(i)}_b\theta))}{d\theta_j} = y^{(i)}(1-\sigma(X^{(i)}_b\theta))\cdot X^{(i)}_j\\
\frac{d((1-y^{(i)})\log(1-\sigma(X^{(i)}_b\theta)))}{d\theta_j} = (1-y^{(i)})\cdot (-\sigma(X^{(i)}_b\theta))\cdot X^{(i)}_j\\
\frac{J(\theta)}{\theta_j} = \frac{1}{m}\sum_{i=1}^m(\sigma(X^{(i)}_b\theta) - y^{(i)})X^{(i)}_j\\
=\frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})X^{(i)}_j\\
\nabla J(\theta) = \frac{1}{m}\left\{\begin{matrix}
  \sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)}) \\
  \sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})X^{(i)}_1 \\
  \sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})X^{(i)}_2 \\
  \cdots \\
  \sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})X^{(i)}_n
\end{matrix}\right\} = \frac{1}{m}\cdot X^T_b\cdot(\sigma(X_b\theta) - y)
$$