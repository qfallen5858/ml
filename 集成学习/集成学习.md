# 什么是集成学习

多个算法集中起来对一个问题进行运算，然后投票决定结果

- kNN
- 逻辑回归
- SVM
- 决策树
- 神经网络
- 贝叶斯

## hard Voting

投票：少数服从多数

Voting Classifier

## soft voting

更合理的投票，应该有权值

将不同算法的分类结果概率作为权值

要求集合的每一个模型都能估计概率 predict_proba

## Bagging 和Pasting

每个子模型只看样本数据的一部分，创建差异性

如何创建差异性？

取样：放回取样 or 不放回取样

放回取样Bagging 不放回取样：Pasting

Bagging更常用

统计学中，放回取样：bootstrap

### Bagging的更多讨论

OOB Out-of-Bag：描述了放回取样会导致一部分样本很有可能没有渠道，平均大约有37%的样本没有取到。通常不适用测试数据集，而使用这部分没有取到的样本做测试/验证

oob_score_ 表征测试数据集的得分

Bagging的思路极易进行并行化处理

n_jobs 为多核参数

针对特征进行随机采样 Random Subspaces,既针对样本，又针对特征进行随机采样，Random Patches

## 随机森林

- Bagging
- Bagging Estimator: Decision Tree
- 决策树在节点划分上，在随机的特征子集上寻找最优划分特征

## Extra-Trees

- Bagging
- Bagging Estimator: Decision Tree
- 决策树在节点划分上，使用随机的特征和随机的阈值
- 提供额外的随机性，抑制过拟合，但增大了bias(偏差)
- 更快的训练速度

## Boosting

- 集成多个模型
- 每个模型都在尝试增强（boosting）整体的效果

### Ada Boosting

串行增强，初始值样本点权值一样，根据每次模型的结果来调整样本点的权值

### Gradient Boosting

1. 训练一个模型m1，产生错误e1
2. 针对e1训练第二个模型m2，产生错误e2
3. 针对e2训练第二个模型m3,产生错误e3
4. 最终预测结果是m1+m2+m3

内部就是用的决策树算法

## Stacking

比voting相比，会在voting集作为输入再分类一次，还可以在交叉训练

类似于神经网络，容易过拟合，由于训练层数过多

